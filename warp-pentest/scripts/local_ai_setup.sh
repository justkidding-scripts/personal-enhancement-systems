#!/bin/bash
# Local AI Setup Script - Bypassing Token Requirements
# Author: xx
# Purpose: Establish local AI inference for security research

set -e

INSTALL_DIR="/home/nike/local-ai"
mkdir -p "$INSTALL_DIR"

echo "=== Local AI Setup for Token-Free Operation ==="

# Check system resources
echo "--- System Resource Check ---"
RAM_GB=$(free -g | awk 'NR==2{print $2}')
CPU_CORES=$(nproc)
GPU_INFO=$(lspci | grep -i nvidia || echo "No NVIDIA GPU detected")

echo "RAM: ${RAM_GB}GB"
echo "CPU Cores: $CPU_CORES"
echo "GPU: $GPU_INFO"

# Recommend model based on resources
if [ "$RAM_GB" -ge 16 ]; then
    RECOMMENDED_MODEL="llama3.1:8b"
    echo "Recommended model: $RECOMMENDED_MODEL (8B parameters)"
elif [ "$RAM_GB" -ge 8 ]; then
    RECOMMENDED_MODEL="llama3.2:3b"
    echo "Recommended model: $RECOMMENDED_MODEL (3B parameters)"
else
    RECOMMENDED_MODEL="tinyllama:1.1b"
    echo "Recommended model: $RECOMMENDED_MODEL (1.1B parameters - lightweight)"
fi

# Install Ollama if not present
echo "--- Installing Ollama ---"
if ! command -v ollama &> /dev/null; then
    curl -fsSL https://ollama.ai/install.sh | sh
    echo "Ollama installed successfully"
else
    echo "Ollama already installed"
fi

# Start Ollama service
echo "--- Starting Ollama Service ---"
ollama serve &
OLLAMA_PID=$!
sleep 5

# Download recommended model
echo "--- Downloading Model: $RECOMMENDED_MODEL ---"
ollama pull "$RECOMMENDED_MODEL"

# Verify model is working
echo "--- Testing Local Model ---"
ollama run "$RECOMMENDED_MODEL" "Hello, respond with exactly 'Local AI active'"

echo "--- Setting up OpenAI-Compatible Proxy ---"
pip3 install --user litellm

# Create proxy configuration
cat > "$INSTALL_DIR/proxy_config.py" << 'EOF'
import litellm
from litellm import completion
import uvicorn
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import json

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/v1/chat/completions")
async def chat_completions(request: dict):
    try:
        # Route to local Ollama
        response = completion(
            model="ollama/llama3.1:8b",
            messages=request.get("messages", []),
            api_base="http://localhost:11434"
        )
        return response
    except Exception as e:
        return {"error": str(e)}

@app.get("/v1/models")
async def list_models():
    return {
        "data": [
            {
                "id": "llama3.1:8b",
                "object": "model",
                "created": 1234567890,
                "owned_by": "local"
            }
        ]
    }

if __name__ == "__main__":
    uvicorn.run(app, host="127.0.0.1", port=8080)
EOF

# Create service file for persistence
cat > "$INSTALL_DIR/start_proxy.sh" << 'EOF'
#!/bin/bash
cd /home/nike/local-ai
ollama serve &
sleep 3
python3 proxy_config.py &
echo "Local AI proxy running on http://127.0.0.1:8080"
echo "OpenAI-compatible endpoint: http://127.0.0.1:8080/v1/chat/completions"
EOF

chmod +x "$INSTALL_DIR/start_proxy.sh"

echo "--- Creating Warp Configuration Override ---"
WARP_CONFIG_DIR="$HOME/.config/warp"
mkdir -p "$WARP_CONFIG_DIR"

# Backup existing config
if [ -f "$WARP_CONFIG_DIR/config.json" ]; then
    cp "$WARP_CONFIG_DIR/config.json" "$WARP_CONFIG_DIR/config.json.backup"
fi

# Create new config pointing to local endpoint
cat > "$WARP_CONFIG_DIR/local_ai_config.json" << 'EOF'
{
  "ai_endpoint": "http://127.0.0.1:8080/v1/chat/completions",
  "ai_api_key": "local-key-not-required",
  "model": "llama3.1:8b",
  "local_mode": true,
  "bypass_auth": true
}
EOF

echo "--- Installation Complete ---"
echo "To use local AI without tokens:"
echo "1. Run: $INSTALL_DIR/start_proxy.sh"
echo "2. Set environment variables:"
echo "   export OPENAI_API_BASE=http://127.0.0.1:8080/v1"
echo "   export OPENAI_API_KEY=local-key-not-required"
echo "3. Or modify Warp config to use local endpoint"

# Create test script
cat > "$INSTALL_DIR/test_local_ai.sh" << 'EOF'
#!/bin/bash
echo "Testing local AI endpoint..."
curl -X POST http://127.0.0.1:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer local-key-not-required" \
  -d '{
    "model": "llama3.1:8b",
    "messages": [{"role": "user", "content": "Explain JWT tokens in 50 words"}]
  }' | jq .
EOF

chmod +x "$INSTALL_DIR/test_local_ai.sh"

echo "--- Test script created: $INSTALL_DIR/test_local_ai.sh ---"
echo "--- Setup complete. Local AI ready for token-free operation ---"